{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Segmentation of cell nucleii"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2 \n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional ( I like QT Graphs so i can zoom and i think %matplotlib widget sucks!)\n",
    "%matplotlib qt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data\n",
    "\n",
    "Data comes from the git repo through [git lfs](https://git-lfs.github.com/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = []\n",
    "\n",
    "for imagePath in Path(\"./data\").glob(\"*.png\"):\n",
    "    image = cv2.imread(str(imagePath))\n",
    "    if image.size > 0:\n",
    "        images.append(image)\n",
    "    else:\n",
    "        print(f\"Failed reading image {imagePath}\")\n",
    "\n",
    "images = np.stack(images)\n",
    "N = len(images)\n",
    "print(images.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def colPlot(images, **kwargs):\n",
    "    fig, axs = plt.subplots(1,len(images))\n",
    "\n",
    "    for im, ax in zip(images, axs):\n",
    "        if im.ndim > 2: # if color\n",
    "            ax.imshow(im[...,::-1]) # opencv is BGR\n",
    "        else:\n",
    "            ax.imshow(im, **kwargs)\n",
    "    \n",
    "    fig.tight_layout()\n",
    "\n",
    "def multiRowPlot(images, titles, nrows, ncols, **kwargs):\n",
    "    fig, axs = plt.subplots(nrows, ncols, sharey=\"col\", sharex=\"col\")\n",
    "\n",
    "    for i, (im, title, ax) in enumerate(zip(images, titles, axs.flatten())):\n",
    "        ax.set_title(title)\n",
    "        ax.imshow(im, **kwargs)\n",
    "        ax.axis(\"off\")\n",
    "\n",
    "    fig.tight_layout()\n",
    "\n",
    "def imageTitles(pattern):\n",
    "    return [pattern.format(i=i) for i in range(1, N+1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1\n",
    "\n",
    "First we analyse the channels of the image and pick the best way to \"grayscale\" it.\n",
    "\n",
    "The red channel is a highliting of cell nuceii, and the G and B channels (equivalent) are the grayscale, monochromatic image from the microscope."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot all channels (RGB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colPlot(images[:,:,::-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot red channel and R-B for comparrison\n",
    "\n",
    "* $I_R-I_B$ clearly shows nucleii with high contrast and no unwanted features.\n",
    "* $I_R$ shows more detail for other parts of the cell. But that detail introduces unwanted features that don't have a very high contrast with nucleii\n",
    "\n",
    "**Use $I_f=I_R-I_B$ for segmentation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the R channel\n",
    "R = images[...,2] \n",
    "\n",
    "# the R-B difference image\n",
    "diffRB = images[...,2] - images[...,0]\n",
    "\n",
    "# For plotting\n",
    "imPlots = chain(R, diffRB)\n",
    "imTitles = chain(imageTitles(\"$I_{{ {i}R }}$\"),\n",
    "        imageTitles(\"$I_{{ {i}R }} - I_{{ {i}B }}$\"))\n",
    "\n",
    "multiRowPlot(imPlots, imTitles, nrows=2, ncols=N, cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Histograms\n",
    "\n",
    "We can also see in a log-histogram that $I_R-I_B$ has a more more distinct peak in its histogram, meaning higher contrast between backgrond and features.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def histColPlot(images:np.array, hist_args:dict):\n",
    "    fig, axs = plt.subplots(2,len(images))\n",
    "    for i, (im, ax) in enumerate(zip(images, axs[0])):\n",
    "        ax.set_title(f\"Image {i+1}\")\n",
    "        ax.imshow(im, cmap=\"gray\")\n",
    "        ax.axis(\"off\")\n",
    "\n",
    "    for i, (im, ax) in enumerate(zip(images, axs[1])):\n",
    "        ax.set_title(f\"Image {i+1} hist\")\n",
    "        ax.hist(im.flatten(), **hist_args)\n",
    "\n",
    "    fig.tight_layout()\n",
    "\n",
    "    return fig, axs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "histColPlot(R, \n",
    "            hist_args=dict(bins=255,log=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "histColPlot(diffRB, \n",
    "            hist_args=dict(bins=255,log=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Determine $I_f$\n",
    "\n",
    "$I_f$ is the grayscale image we'll use for segmentation. We do a linear stretching of the $I_R-I_B$ image so that $max(I_R-I_B)=255$ and $min(I_R-I_B)=0$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "useChannel = diffRB\n",
    "\n",
    "# np.max is calculated over ALL images\n",
    "# This means e.g. we don't strech image 1 more than image 2\n",
    "minval = np.min(useChannel)\n",
    "If = (useChannel - minval) * 255.0 / (np.max(useChannel)-minval)\n",
    "\n",
    "If = If.astype(np.uint8)\n",
    "\n",
    "histColPlot(If, hist_args=dict(bins=255, log=True))\n",
    "plt.suptitle(\"$I_f$ and Histograms\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2 - Thresholding\n",
    "\n",
    "We apply two thresholding techniques to segment the nucleii of cells using $I_f$.\n",
    "\n",
    "1. \"handmade\" threshold: from the histograms, we choose $T$ such that is isolates the background\n",
    "2. Otsu's technique: we use the opencv implementation of Otsu's thresholding to determine $T$\n",
    "\n",
    "We then extract the nucleii count and areas using `cv2.connectedComponents`, for result analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "histColPlot(If, hist_args=dict(bins=255, log=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Threshold: Manual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feels like this is a good value:\n",
    "T = 40\n",
    "fig, axs = histColPlot(If, hist_args=dict(bins=255, log=False))\n",
    "\n",
    "for hist in axs[1]:\n",
    "    hist.axvline(T, color=\"r\", label=f\"{T=}\")\n",
    "    hist.legend()\n",
    "plt.suptitle(\"$I_f$ histograms and chosen Threshold\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshManual = 255* ( If > T )\n",
    "threshManual=threshManual.astype(np.uint8)\n",
    "\n",
    "imPlots = chain(If, threshManual)\n",
    "imTitles = chain(imageTitles(\"$I_f{{ {i} }}$\"), imageTitles(\"$T_{i}$\"))\n",
    "\n",
    "multiRowPlot(imPlots, imTitles, nrows=2, ncols=N, cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Threshold: Otsu`s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshOtsu = []\n",
    "threshOtsuVals = []\n",
    "for If_i in If:\n",
    "    ret, otsuThresh_i =  cv2.threshold(If_i, 127, 255, cv2.THRESH_OTSU | cv2.THRESH_BINARY)\n",
    "    threshOtsu.append(otsuThresh_i)\n",
    "    threshOtsuVals.append(ret)\n",
    "\n",
    "threshOtsu=np.stack(threshOtsu, axis=0)\n",
    "\n",
    "imPlots = chain(images, threshManual, threshOtsu)\n",
    "imTitles = chain(imageTitles(\"$I_f{{ {i} }}$\"), \n",
    "    (\n",
    "      title + f\",{T=}\" \n",
    "      for title in imageTitles(\"$T_{i} Manual$\")\n",
    "    ),\n",
    "    ( title + f\", {T=}\"\n",
    "      for T, title \n",
    "      in zip(threshOtsuVals,imageTitles(\"$T_{i}$ Otsu\"))\n",
    "    ))\n",
    "\n",
    "print(imTitles)\n",
    "multiRowPlot(imPlots, imTitles, nrows=3, ncols=N, cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation\n",
    "\n",
    "`ThreshManual` looks more consistant overall, with threshOtsu choosing a value that is slightly too high and leaves out part of some cells."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This makes sense because `Otsu` assumes a strong **bimodal distribution**, but the image has a very small foreground area. \n",
    "\n",
    "We can see this by calculating the percentage of pixels labeled as foreground by manual thresholding:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of foreground pixels (by image):\")\n",
    "print(np.count_nonzero(threshManual, axis=(1,2)))\n",
    "print(\"Percent of foreground pixels (by image):\")\n",
    "percents = np.round(np.count_nonzero(threshManual, axis=(1,2))/threshManual[0].size * 100, 2)\n",
    "print(\", \".join([f\"{p}%\" for p in percents]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We picked this threshold:\n",
    "thresh = threshManual"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get connected areas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = 8\n",
    "areas = []\n",
    "for image in thresh:\n",
    "    n_labels, labels =  cv2.connectedComponents(image, None, conn)\n",
    "\n",
    "    # count number of pixels in each connected component\n",
    "    # print(np.array([labels==i for i in range(n_labels)]).shape)\n",
    "    area = np.count_nonzero([labels==i for i in range(n_labels)], axis=(1,2))\n",
    "\n",
    "    # First item is background label\n",
    "    area = area[1:]\n",
    "\n",
    "    areas.append(area)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* There are a lot of areas with 1 or 2 pixels. \n",
    "    These are likely noise.\n",
    "\n",
    "    We'll calculate stats including them first, but later we'll try filtering out areas that equal 1 or 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, area in enumerate(areas):\n",
    "    print(f\"Image {i+1} areas:\")\n",
    "    print(area)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stats\n",
    "\n",
    "**Without filtering** \n",
    "* We see there are a lot of 1-pixel areas (over 25% for some images, over 50% for image 1) .\n",
    "* Maybe because of this, the mean area for the first image is much smaller\n",
    "* The biggest nucleus is in image 3, which also has the highest mean size. But the median size of cells is higher on image4\n",
    "\n",
    "**With filetring**\n",
    "* Now the mean size of images is more similar, with images 3 and 4 having the bigger cells\n",
    "* Standard deviation is very high, so cells are varying a lot in size.\n",
    "* We see this on the quartiles, with 20% of cells being smaller than 50 pixels on all images but image 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_stats(areas):\n",
    "    dfs = [pd.DataFrame(data={f\"image{i+1}\":area}) for i, area in enumerate(areas)]\n",
    "    print(pd.concat([df.describe().transpose().round(2) for df in dfs]))\n",
    "\n",
    "print(\"====== ALL\")\n",
    "print_stats(areas)\n",
    "\n",
    "min_area=5\n",
    "print(f\"====== Areas > {min_area=}\")\n",
    "print_stats([a[a>min_area] for a in areas])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "51e85f04e04be4d88eed80c0e07765bbdfe0d9abe501b20be23dd8c1d7c4662d"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 (conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
