{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Segmentation of cell nucleii"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2 \n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional ( I like QT Graphs so i can zoom and i think %matplotlib widget sucks!)\n",
    "%matplotlib qt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data\n",
    "\n",
    "Data comes from the git repo through [git lfs](https://git-lfs.github.com/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = []\n",
    "\n",
    "for imagePath in Path(\"./data\").glob(\"*.png\"):\n",
    "    image = cv2.imread(str(imagePath))\n",
    "    if image.size > 0:\n",
    "        images.append(image)\n",
    "    else:\n",
    "        print(f\"Failed reading image {imagePath}\")\n",
    "\n",
    "images = np.stack(images)\n",
    "N = len(images)\n",
    "print(images.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maximized=True\n",
    "def colPlot(images, **kwargs):\n",
    "    fig, axs = plt.subplots(1,len(images))\n",
    "\n",
    "    for im, ax in zip(images, axs):\n",
    "        if im.ndim > 2: # if color\n",
    "            ax.imshow(im[...,::-1]) # opencv is BGR\n",
    "        else:\n",
    "            ax.imshow(im, **kwargs)\n",
    "    \n",
    "    if maximized:\n",
    "        figManager = plt.get_current_fig_manager()\n",
    "        figManager.window.showMaximized()\n",
    "    fig.tight_layout()\n",
    "\n",
    "def multiRowPlot(images, titles, nrows, ncols, **kwargs):\n",
    "    fig, axs = plt.subplots(nrows, ncols, sharey=\"col\", sharex=\"col\")\n",
    "\n",
    "    for i, (im, title, ax) in enumerate(zip(images, titles, axs.flatten())):\n",
    "        ax.set_title(title)\n",
    "        ax.imshow(im, **kwargs)\n",
    "        ax.axis(\"off\")\n",
    "\n",
    "\n",
    "    if maximized:\n",
    "        figManager = plt.get_current_fig_manager()\n",
    "        figManager.window.showMaximized()\n",
    "    fig.tight_layout()\n",
    "\n",
    "\n",
    "def imageTitles(pattern):\n",
    "    return [pattern.format(i=i) for i in range(1, N+1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1\n",
    "\n",
    "First we analyse the channels of the image and pick the best way to \"grayscale\" it.\n",
    "\n",
    "The red channel is a highliting of cell nuceii, and the G and B channels (equivalent) are the grayscale, monochromatic image from the microscope."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot all channels (RGB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colPlot(images[:,:,::-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot red channel and R-B for comparrison\n",
    "\n",
    "* $I_R-I_B$ clearly shows nucleii with high contrast and no unwanted features.\n",
    "* $I_R$ shows more detail for other parts of the cell. But that detail introduces unwanted features that don't have a very high contrast with nucleii\n",
    "\n",
    "**Use $I_f=I_R-I_B$ for segmentation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the R channel\n",
    "R = images[...,2] \n",
    "\n",
    "# the R-B difference image\n",
    "diffRB = images[...,2] - images[...,0]\n",
    "\n",
    "# For plotting\n",
    "imPlots = chain(R, diffRB)\n",
    "imTitles = chain(imageTitles(\"$I_{{ {i}R }}$\"),\n",
    "        imageTitles(\"$I_{{ {i}R }} - I_{{ {i}B }}$\"))\n",
    "\n",
    "multiRowPlot(imPlots, imTitles, nrows=2, ncols=N, cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for a,b in zip(images,diffRB):\n",
    "    fig, axs = plt.subplots(1,2,sharex=True,sharey=True)\n",
    "    axs[0].imshow(a[...,0], cmap=\"gray\")\n",
    "    axs[1].imshow(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Histograms\n",
    "\n",
    "We can also see in a log-histogram that $I_R-I_B$ has a more more distinct peak in its histogram, meaning higher contrast between backgrond and features.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def histColPlot(images:np.array, hist_args:dict):\n",
    "    fig, axs = plt.subplots(2,len(images))\n",
    "    for i, (im, ax) in enumerate(zip(images, axs[0])):\n",
    "        ax.set_title(f\"Image {i+1}\")\n",
    "        ax.imshow(im, cmap=\"gray\")\n",
    "        ax.axis(\"off\")\n",
    "\n",
    "    for i, (im, ax) in enumerate(zip(images, axs[1])):\n",
    "        ax.set_title(f\"Image {i+1} hist\")\n",
    "        ax.hist(im.flatten(), **hist_args)\n",
    "\n",
    "    fig.tight_layout()\n",
    "\n",
    "    return fig, axs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "histColPlot(R, \n",
    "            hist_args=dict(bins=255,log=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "histColPlot(diffRB, \n",
    "            hist_args=dict(bins=255,log=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Determine $I_f$\n",
    "\n",
    "$I_f$ is the grayscale image we'll use for segmentation. We do a linear stretching of the $I_R-I_B$ image so that $max(I_R-I_B)=255$ and $min(I_R-I_B)=0$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "useChannel = diffRB\n",
    "\n",
    "# np.max is calculated over ALL images\n",
    "# This means e.g. we don't strech image 1 more than image 2\n",
    "minval = np.min(useChannel)\n",
    "If = (useChannel - minval) * 255.0 / (np.max(useChannel)-minval)\n",
    "\n",
    "If = If.astype(np.uint8)\n",
    "\n",
    "histColPlot(If, hist_args=dict(bins=255, log=True))\n",
    "plt.suptitle(\"$I_f$ and Histograms\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2 - Thresholding\n",
    "\n",
    "We apply two thresholding techniques to segment the nucleii of cells using $I_f$.\n",
    "\n",
    "1. \"handmade\" threshold: from the histograms, we choose $T$ such that is isolates the background\n",
    "2. Otsu's technique: we use the opencv implementation of Otsu's thresholding to determine $T$\n",
    "\n",
    "We then extract the nucleii count and areas using `cv2.connectedComponents`, for result analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "histColPlot(If, hist_args=dict(bins=255, log=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Threshold: Manual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feels like this is a good value:\n",
    "T = 35"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = histColPlot(If, hist_args=dict(bins=255, log=True))\n",
    "\n",
    "for hist in axs[1]:\n",
    "    hist.axvline(T, color=\"r\", label=f\"{T=}\")\n",
    "    hist.legend()\n",
    "plt.suptitle(\"$I_f$ histograms and chosen Threshold\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshManual = 255* ( If > T )\n",
    "threshManual=threshManual.astype(np.uint8)\n",
    "\n",
    "imPlots = chain(If, threshManual)\n",
    "imTitles = chain(imageTitles(\"$I_f{{ {i} }}$\"), imageTitles(\"$T_{i}$\"))\n",
    "\n",
    "multiRowPlot(imPlots, imTitles, nrows=2, ncols=N, cmap=\"gray\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Threshold: Otsu`s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshOtsu = []\n",
    "threshOtsuVals = []\n",
    "for If_i in If:\n",
    "    ret, otsuThresh_i =  cv2.threshold(If_i, 127, 255, cv2.THRESH_OTSU | cv2.THRESH_BINARY)\n",
    "    threshOtsu.append(otsuThresh_i)\n",
    "    threshOtsuVals.append(ret)\n",
    "\n",
    "threshOtsu=np.stack(threshOtsu, axis=0)\n",
    "\n",
    "imPlots = chain(If, threshManual, threshOtsu)\n",
    "imTitles = chain(imageTitles(\"$I_f{{ {i} }}$\"), \n",
    "    (\n",
    "      title + f\",{T=}\" \n",
    "      for title in imageTitles(\"$T_{i} Manual$\")\n",
    "    ),\n",
    "    ( title + f\", {T=}\"\n",
    "      for T, title \n",
    "      in zip(threshOtsuVals,imageTitles(\"$T_{i}$ Otsu\"))\n",
    "    ))\n",
    "\n",
    "print(imTitles)\n",
    "multiRowPlot(imPlots, imTitles, nrows=3, ncols=N, cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation\n",
    "\n",
    "`ThreshManual` looks more consistant overall, with `threshOtsu` choosing a value that is slightly too high and leaves out part of some cells."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This makes sense because `Otsu` assumes a strong **bimodal distribution**, but the image has a very small foreground area. \n",
    "\n",
    "We can see this by calculating the percentage of pixels labeled as foreground by manual thresholding:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of foreground pixels (by image):\")\n",
    "print(np.count_nonzero(threshManual, axis=(1,2)))\n",
    "print(\"Percent of foreground pixels (by image):\")\n",
    "percents = np.round(np.count_nonzero(threshManual, axis=(1,2))/threshManual[0].size * 100, 2)\n",
    "print(\", \".join([f\"{p}%\" for p in percents]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We picked this threshold:\n",
    "thresh = threshManual"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get connected areas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = 8\n",
    "areas = []\n",
    "for image in thresh:\n",
    "    n_labels, labels =  cv2.connectedComponents(image, None, conn)\n",
    "\n",
    "    # count number of pixels in each connected component\n",
    "    # print(np.array([labels==i for i in range(n_labels)]).shape)\n",
    "    area = np.count_nonzero([labels==i for i in range(n_labels)], axis=(1,2))\n",
    "\n",
    "    # First item is background label\n",
    "    area = area[1:]\n",
    "\n",
    "    areas.append(area)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* There are a lot of areas with 1 or 2 pixels. \n",
    "    These are likely noise.\n",
    "\n",
    "    We'll calculate stats including them first, but later we'll try filtering out areas that equal 1 or 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, area in enumerate(areas):\n",
    "    print(f\"Image {i+1} areas:\")\n",
    "    print(area)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stats\n",
    "\n",
    "**Original** \n",
    "* We see there are a lot of 1-pixel areas (over 25% for some images, over 50% for image 1) .\n",
    "* Maybe because of this, the mean area for the first image is much smaller\n",
    "* The biggest nucleus is in image 3, which also has the highest mean size. But the median size of cells is higher on image4\n",
    "\n",
    "**With minimum area**\n",
    "* Now the mean size of images is more similar, with images 3 and 4 having the bigger cells\n",
    "* Standard deviation is very high, so cells are varying a lot in size.\n",
    "* We see this on the quartiles, with 20% of cells being smaller than 50 pixels on all images but image 4\n",
    "\n",
    "**Comparison with manual count**\n",
    "* Manual count is more similar to the minimum area values. \n",
    "* But either way these counts are different from the manual count. Mainly because of \"close-together\" nucleii being counted as only one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_stats(areas):\n",
    "    dfs = [pd.DataFrame(data={f\"image{i+1}\":area}) for i, area in enumerate(areas)]\n",
    "    print(pd.concat([df.describe().transpose().round(2) for df in dfs]))\n",
    "\n",
    "def get_areas_and_print_stats(images):\n",
    "    areas = []\n",
    "    for i, image in enumerate(images):\n",
    "        n_labels, labels =  cv2.connectedComponents(image, None, conn)\n",
    "        # count number of pixels in each connected component\n",
    "        # print(np.array([labels==i for i in range(n_labels)]).shape)\n",
    "        area = np.count_nonzero([labels==i for i in range(n_labels)], axis=(1,2))\n",
    "        # First item is background label\n",
    "        area = area[1:]\n",
    "        areas.append(area)\n",
    "    \n",
    "    print_stats(areas)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"====== ALL\")\n",
    "print_stats(areas)\n",
    "\n",
    "min_area=5\n",
    "print(f\"====== Areas > {min_area=}\")\n",
    "print_stats([a[a>min_area] for a in areas])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Watershet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient\n",
    "(need to study this)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gradients=[]\n",
    "for If_i in If:\n",
    "    ddepth = cv2.CV_32F\n",
    "    \n",
    "    dx = cv2.Sobel(f, ddepth, 1, 0)\n",
    "    dy = cv2.Sobel(f, ddepth, 0, 1)\n",
    "\n",
    "    gradients.append(np.sqrt(dx**2+dy**2))\n",
    "gradients=np.stack(gradients,axis=0)\n",
    "\n",
    "multiRowPlot(\n",
    "    chain(If, gradients),\n",
    "    chain(imageTitles(\"$ I_{{ f{i} }} $\"), imageTitles(\"$ | \\\\Delta I_{{ f{i} }} | $\")),\n",
    "    2,\n",
    "    N\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Markers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel = np.ones((3,3))\n",
    "markers = [cv2.morphologyEx(m, cv2.MORPH_CLOSE, kernel) for m in thresh]\n",
    "markers = [cv2.erode(m,  kernel) for m in markers]\n",
    "markers = np.stack(markers, axis=0)\n",
    "\n",
    "kernel = np.ones((5,5))\n",
    "bgs = [~cv2.dilate(m,  kernel, iterations=3) for m in thresh]\n",
    "bgs = np.stack(bgs, axis=0)\n",
    "\n",
    "plt_images = chain(thresh, markers, bgs)\n",
    "plt_titles = chain(imageTitles(\"thresh\"), imageTitles(\"marker\"), imageTitles(\"background\"))\n",
    "multiRowPlot(plt_images , plt_titles , 3, N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gradients = gradients*255/gradients.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.segmentation import watershed\n",
    "\n",
    "watersheds=[]\n",
    "for m, bg, grad in zip(markers, bgs, gradients):\n",
    "    conn = 8\n",
    "    n_labels, labels =  cv2.connectedComponents(m, None, conn)\n",
    "\n",
    "    labels[bg>0] = labels.max() + 1\n",
    "    # plt.imshow(labels * 255 / labels.max())\n",
    "    out = watershed(grad, labels.astype(np.int32))\n",
    "    out[out==out.max()] = 0\n",
    "    watersheds.append(out)\n",
    "watersheds=np.stack(watersheds,axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multiRowPlot(\n",
    "    chain(If, markers, watersheds),\n",
    "    chain(imageTitles(\"$ I_{{ f{i} }} $\"), imageTitles(\"$marker_{{ B{i} }}$\"), imageTitles(\"$watershed_{{ {i} }}$\")),\n",
    "    3,\n",
    "    N\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"====== ALL\")\n",
    "print_stats(areas)\n",
    "\n",
    "min_area=5\n",
    "print(f\"====== Areas > {min_area=}\")\n",
    "print_stats([a[a>min_area] for a in areas])\n",
    "print(f\"====== watershed\")\n",
    "get_areas_and_print_stats(255*(watersheds>0).astype(np.uint8))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "51e85f04e04be4d88eed80c0e07765bbdfe0d9abe501b20be23dd8c1d7c4662d"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 (conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
